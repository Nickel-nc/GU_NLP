{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Переводчик на seq2seq Attention Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "epochs = 100\n",
    "latent_dim = 256\n",
    "num_samples = 30000\n",
    "data_path = 'data/rus-eng/rus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tМарш!\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1159202 (shanghainese)\n",
      "Go.\tИди.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #5898247 (marafon)\n",
      "Go.\tИдите.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #5898250 (marafon)\n",
      "Hi.\tЗдравствуйте.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #402127 (odexed)\n",
      "Hi.\tПривет!\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #466968 (katjka)\n",
      "Hi.\tХай.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #467233 (timsa)\n",
      "Hi.\tЗдрасте.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #3803577 (marafon)\n",
      "Hi.\tЗдоро́во!\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #3854188 (marafon)\n",
      "Run!\tБеги!\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #1569978 (Biga)\n",
      "Run!\tБегите!\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #2770234 (marafon)\n",
      "Run.\tБеги!\tCC-BY 2.0 (France) Attribution: tatoeba.org #4008918 (JSakuragi) & #1569978 (Bi\n"
     ]
    }
   ],
   "source": [
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read()[:1000]\n",
    "    print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Препроцессинг, очистка текста в последовательности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собираем из текстов токены и делаем one-hot вектора на каждый токен\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задаем размеры последовательности\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение базового seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестовая генерация. В данной модели в LSTM-ячейку подается посимвольная последовательность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/100\n",
      "24000/24000 [==============================] - 8s 329us/step - loss: 2.1781 - accuracy: 0.7215 - val_loss: 1.4754 - val_accuracy: 0.7248\n",
      "Epoch 2/100\n",
      "24000/24000 [==============================] - 7s 291us/step - loss: 1.2270 - accuracy: 0.7554 - val_loss: 1.2855 - val_accuracy: 0.7251\n",
      "Epoch 3/100\n",
      "24000/24000 [==============================] - 7s 291us/step - loss: 1.0927 - accuracy: 0.7541 - val_loss: 1.2066 - val_accuracy: 0.7242\n",
      "Epoch 4/100\n",
      "24000/24000 [==============================] - 7s 293us/step - loss: 1.0233 - accuracy: 0.7507 - val_loss: 1.1139 - val_accuracy: 0.7245\n",
      "Epoch 5/100\n",
      "24000/24000 [==============================] - 7s 293us/step - loss: 0.9516 - accuracy: 0.7559 - val_loss: 1.0449 - val_accuracy: 0.7278\n",
      "Epoch 6/100\n",
      "24000/24000 [==============================] - 7s 293us/step - loss: 0.9028 - accuracy: 0.7579 - val_loss: 1.0027 - val_accuracy: 0.7260\n",
      "Epoch 7/100\n",
      "24000/24000 [==============================] - 7s 293us/step - loss: 0.8713 - accuracy: 0.7597 - val_loss: 0.9728 - val_accuracy: 0.7282\n",
      "Epoch 8/100\n",
      "24000/24000 [==============================] - 7s 294us/step - loss: 0.8428 - accuracy: 0.7655 - val_loss: 0.9469 - val_accuracy: 0.7411\n",
      "Epoch 9/100\n",
      "24000/24000 [==============================] - 7s 294us/step - loss: 0.8181 - accuracy: 0.7783 - val_loss: 0.9208 - val_accuracy: 0.7539\n",
      "Epoch 10/100\n",
      "24000/24000 [==============================] - 7s 294us/step - loss: 0.7931 - accuracy: 0.7892 - val_loss: 0.8946 - val_accuracy: 0.7640\n",
      "Epoch 11/100\n",
      "24000/24000 [==============================] - 7s 295us/step - loss: 0.7662 - accuracy: 0.7983 - val_loss: 0.8648 - val_accuracy: 0.7717\n",
      "Epoch 12/100\n",
      "24000/24000 [==============================] - 7s 294us/step - loss: 0.7367 - accuracy: 0.8062 - val_loss: 0.8311 - val_accuracy: 0.7815\n",
      "Epoch 13/100\n",
      "24000/24000 [==============================] - 7s 299us/step - loss: 0.7085 - accuracy: 0.8114 - val_loss: 0.8044 - val_accuracy: 0.7841\n",
      "Epoch 14/100\n",
      "24000/24000 [==============================] - 7s 297us/step - loss: 0.6838 - accuracy: 0.8154 - val_loss: 0.7749 - val_accuracy: 0.7893\n",
      "Epoch 15/100\n",
      "24000/24000 [==============================] - 7s 300us/step - loss: 0.6604 - accuracy: 0.8192 - val_loss: 0.7529 - val_accuracy: 0.7939\n",
      "Epoch 16/100\n",
      "24000/24000 [==============================] - 7s 295us/step - loss: 0.6419 - accuracy: 0.8220 - val_loss: 0.7331 - val_accuracy: 0.7970\n",
      "Epoch 17/100\n",
      "24000/24000 [==============================] - 7s 294us/step - loss: 0.6259 - accuracy: 0.8245 - val_loss: 0.7178 - val_accuracy: 0.8005\n",
      "Epoch 18/100\n",
      "24000/24000 [==============================] - 7s 295us/step - loss: 0.6159 - accuracy: 0.8262 - val_loss: 0.7029 - val_accuracy: 0.8007\n",
      "Epoch 19/100\n",
      "24000/24000 [==============================] - 7s 295us/step - loss: 0.6012 - accuracy: 0.8293 - val_loss: 0.6907 - val_accuracy: 0.8034\n",
      "Epoch 20/100\n",
      "24000/24000 [==============================] - 7s 295us/step - loss: 0.5902 - accuracy: 0.8319 - val_loss: 0.6773 - val_accuracy: 0.8071\n",
      "Epoch 21/100\n",
      "24000/24000 [==============================] - 7s 288us/step - loss: 0.5798 - accuracy: 0.8345 - val_loss: 0.6678 - val_accuracy: 0.8098\n",
      "Epoch 22/100\n",
      "24000/24000 [==============================] - 7s 282us/step - loss: 0.5710 - accuracy: 0.8366 - val_loss: 0.6585 - val_accuracy: 0.8127\n",
      "Epoch 23/100\n",
      "24000/24000 [==============================] - 7s 282us/step - loss: 0.5627 - accuracy: 0.8390 - val_loss: 0.6473 - val_accuracy: 0.8149\n",
      "Epoch 24/100\n",
      "24000/24000 [==============================] - 7s 282us/step - loss: 0.5553 - accuracy: 0.8408 - val_loss: 0.6418 - val_accuracy: 0.8175\n",
      "Epoch 25/100\n",
      "24000/24000 [==============================] - 7s 284us/step - loss: 0.5472 - accuracy: 0.8431 - val_loss: 0.6340 - val_accuracy: 0.8198\n",
      "Epoch 26/100\n",
      "24000/24000 [==============================] - 7s 282us/step - loss: 0.5400 - accuracy: 0.8450 - val_loss: 0.6345 - val_accuracy: 0.8192\n",
      "Epoch 27/100\n",
      "24000/24000 [==============================] - 7s 282us/step - loss: 0.5347 - accuracy: 0.8464 - val_loss: 0.6206 - val_accuracy: 0.8226\n",
      "Epoch 28/100\n",
      "24000/24000 [==============================] - 7s 284us/step - loss: 0.5279 - accuracy: 0.8482 - val_loss: 0.6138 - val_accuracy: 0.8243\n",
      "Epoch 29/100\n",
      "24000/24000 [==============================] - 7s 292us/step - loss: 0.5217 - accuracy: 0.8501 - val_loss: 0.6103 - val_accuracy: 0.8252\n",
      "Epoch 30/100\n",
      "24000/24000 [==============================] - 8s 314us/step - loss: 0.5182 - accuracy: 0.8507 - val_loss: 0.6064 - val_accuracy: 0.8260\n",
      "Epoch 31/100\n",
      "24000/24000 [==============================] - 8s 313us/step - loss: 0.5134 - accuracy: 0.8521 - val_loss: 0.6000 - val_accuracy: 0.8273\n",
      "Epoch 32/100\n",
      "24000/24000 [==============================] - 7s 286us/step - loss: 0.5066 - accuracy: 0.8540 - val_loss: 0.5942 - val_accuracy: 0.8289\n",
      "Epoch 33/100\n",
      "24000/24000 [==============================] - 7s 292us/step - loss: 0.5020 - accuracy: 0.8552 - val_loss: 0.5917 - val_accuracy: 0.8293\n",
      "Epoch 34/100\n",
      "24000/24000 [==============================] - 7s 283us/step - loss: 0.4987 - accuracy: 0.8560 - val_loss: 0.5856 - val_accuracy: 0.8309\n",
      "Epoch 35/100\n",
      "24000/24000 [==============================] - 7s 283us/step - loss: 0.4933 - accuracy: 0.8574 - val_loss: 0.5821 - val_accuracy: 0.8324\n",
      "Epoch 36/100\n",
      "24000/24000 [==============================] - 7s 291us/step - loss: 0.4902 - accuracy: 0.8585 - val_loss: 0.5791 - val_accuracy: 0.8326\n",
      "Epoch 37/100\n",
      "24000/24000 [==============================] - 7s 302us/step - loss: 0.4848 - accuracy: 0.8599 - val_loss: 0.5743 - val_accuracy: 0.8341\n",
      "Epoch 38/100\n",
      "24000/24000 [==============================] - 7s 297us/step - loss: 0.4844 - accuracy: 0.8601 - val_loss: 0.5713 - val_accuracy: 0.8351\n",
      "Epoch 39/100\n",
      "24000/24000 [==============================] - 7s 296us/step - loss: 0.4782 - accuracy: 0.8618 - val_loss: 0.5682 - val_accuracy: 0.8358\n",
      "Epoch 40/100\n",
      "24000/24000 [==============================] - 7s 294us/step - loss: 0.4733 - accuracy: 0.8631 - val_loss: 0.5654 - val_accuracy: 0.8370\n",
      "Epoch 41/100\n",
      "24000/24000 [==============================] - 7s 294us/step - loss: 0.4694 - accuracy: 0.8643 - val_loss: 0.5595 - val_accuracy: 0.8386\n",
      "Epoch 42/100\n",
      "24000/24000 [==============================] - 7s 296us/step - loss: 0.4656 - accuracy: 0.8653 - val_loss: 0.5575 - val_accuracy: 0.8393\n",
      "Epoch 43/100\n",
      "24000/24000 [==============================] - 7s 296us/step - loss: 0.4618 - accuracy: 0.8664 - val_loss: 0.5525 - val_accuracy: 0.8400\n",
      "Epoch 44/100\n",
      "24000/24000 [==============================] - 7s 296us/step - loss: 0.4587 - accuracy: 0.8671 - val_loss: 0.5505 - val_accuracy: 0.8406\n",
      "Epoch 45/100\n",
      "24000/24000 [==============================] - 7s 310us/step - loss: 0.4555 - accuracy: 0.8680 - val_loss: 0.5453 - val_accuracy: 0.8426\n",
      "Epoch 46/100\n",
      "24000/24000 [==============================] - 7s 312us/step - loss: 0.4524 - accuracy: 0.8687 - val_loss: 0.5412 - val_accuracy: 0.8444\n",
      "Epoch 47/100\n",
      "24000/24000 [==============================] - 7s 312us/step - loss: 0.4483 - accuracy: 0.8701 - val_loss: 0.5386 - val_accuracy: 0.8445\n",
      "Epoch 48/100\n",
      "24000/24000 [==============================] - 7s 299us/step - loss: 0.4453 - accuracy: 0.8709 - val_loss: 0.5374 - val_accuracy: 0.8450\n",
      "Epoch 49/100\n",
      "24000/24000 [==============================] - 7s 281us/step - loss: 0.4422 - accuracy: 0.8718 - val_loss: 0.5337 - val_accuracy: 0.8460\n",
      "Epoch 50/100\n",
      "24000/24000 [==============================] - 7s 295us/step - loss: 0.4382 - accuracy: 0.8730 - val_loss: 0.5319 - val_accuracy: 0.8472\n",
      "Epoch 51/100\n",
      "24000/24000 [==============================] - 8s 316us/step - loss: 0.4353 - accuracy: 0.8740 - val_loss: 0.5272 - val_accuracy: 0.8480\n",
      "Epoch 52/100\n",
      "24000/24000 [==============================] - 8s 314us/step - loss: 0.4325 - accuracy: 0.8749 - val_loss: 0.5261 - val_accuracy: 0.8487\n",
      "Epoch 53/100\n",
      "24000/24000 [==============================] - 8s 316us/step - loss: 0.4294 - accuracy: 0.8756 - val_loss: 0.5251 - val_accuracy: 0.8487\n",
      "Epoch 54/100\n",
      "24000/24000 [==============================] - 8s 314us/step - loss: 0.4273 - accuracy: 0.8764 - val_loss: 0.5197 - val_accuracy: 0.8503\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000/24000 [==============================] - 8s 313us/step - loss: 0.4231 - accuracy: 0.8776 - val_loss: 0.5188 - val_accuracy: 0.8502\n",
      "Epoch 56/100\n",
      "24000/24000 [==============================] - 7s 294us/step - loss: 0.4205 - accuracy: 0.8783 - val_loss: 0.5139 - val_accuracy: 0.8514\n",
      "Epoch 57/100\n",
      "24000/24000 [==============================] - 7s 309us/step - loss: 0.4174 - accuracy: 0.8792 - val_loss: 0.5124 - val_accuracy: 0.8523\n",
      "Epoch 58/100\n",
      "24000/24000 [==============================] - 8s 317us/step - loss: 0.4146 - accuracy: 0.8800 - val_loss: 0.5091 - val_accuracy: 0.8536\n",
      "Epoch 59/100\n",
      "24000/24000 [==============================] - 8s 318us/step - loss: 0.4118 - accuracy: 0.8808 - val_loss: 0.5068 - val_accuracy: 0.8546\n",
      "Epoch 60/100\n",
      "24000/24000 [==============================] - 8s 317us/step - loss: 0.4097 - accuracy: 0.8813 - val_loss: 0.5068 - val_accuracy: 0.8537\n",
      "Epoch 61/100\n",
      "24000/24000 [==============================] - 8s 318us/step - loss: 0.4077 - accuracy: 0.8818 - val_loss: 0.5043 - val_accuracy: 0.8549\n",
      "Epoch 62/100\n",
      "24000/24000 [==============================] - 8s 322us/step - loss: 0.4046 - accuracy: 0.8828 - val_loss: 0.5043 - val_accuracy: 0.8554\n",
      "Epoch 63/100\n",
      "24000/24000 [==============================] - 7s 307us/step - loss: 0.4024 - accuracy: 0.8837 - val_loss: 0.5012 - val_accuracy: 0.8558\n",
      "Epoch 64/100\n",
      "24000/24000 [==============================] - 7s 299us/step - loss: 0.3994 - accuracy: 0.8843 - val_loss: 0.4987 - val_accuracy: 0.8567\n",
      "Epoch 65/100\n",
      "24000/24000 [==============================] - 8s 319us/step - loss: 0.3976 - accuracy: 0.8847 - val_loss: 0.4966 - val_accuracy: 0.8574\n",
      "Epoch 66/100\n",
      "24000/24000 [==============================] - 8s 319us/step - loss: 0.3947 - accuracy: 0.8858 - val_loss: 0.4964 - val_accuracy: 0.8572\n",
      "Epoch 67/100\n",
      "24000/24000 [==============================] - 8s 317us/step - loss: 0.3922 - accuracy: 0.8863 - val_loss: 0.4922 - val_accuracy: 0.8580\n",
      "Epoch 68/100\n",
      "24000/24000 [==============================] - 8s 315us/step - loss: 0.3898 - accuracy: 0.8870 - val_loss: 0.4916 - val_accuracy: 0.8585\n",
      "Epoch 69/100\n",
      "24000/24000 [==============================] - 8s 319us/step - loss: 0.3877 - accuracy: 0.8877 - val_loss: 0.4884 - val_accuracy: 0.8597\n",
      "Epoch 70/100\n",
      "24000/24000 [==============================] - 8s 315us/step - loss: 0.3849 - accuracy: 0.8884 - val_loss: 0.4863 - val_accuracy: 0.8607\n",
      "Epoch 71/100\n",
      "24000/24000 [==============================] - 7s 312us/step - loss: 0.3823 - accuracy: 0.8892 - val_loss: 0.4847 - val_accuracy: 0.8605\n",
      "Epoch 72/100\n",
      "24000/24000 [==============================] - 8s 314us/step - loss: 0.3806 - accuracy: 0.8897 - val_loss: 0.4860 - val_accuracy: 0.8603\n",
      "Epoch 73/100\n",
      "24000/24000 [==============================] - 8s 313us/step - loss: 0.3794 - accuracy: 0.8899 - val_loss: 0.4853 - val_accuracy: 0.8600\n",
      "Epoch 74/100\n",
      "24000/24000 [==============================] - 8s 314us/step - loss: 0.3772 - accuracy: 0.8907 - val_loss: 0.4808 - val_accuracy: 0.8622\n",
      "Epoch 75/100\n",
      "24000/24000 [==============================] - 7s 311us/step - loss: 0.3736 - accuracy: 0.8917 - val_loss: 0.4802 - val_accuracy: 0.8618\n",
      "Epoch 76/100\n",
      "24000/24000 [==============================] - 8s 313us/step - loss: 0.3722 - accuracy: 0.8920 - val_loss: 0.4799 - val_accuracy: 0.8621\n",
      "Epoch 77/100\n",
      "24000/24000 [==============================] - 8s 313us/step - loss: 0.3704 - accuracy: 0.8926 - val_loss: 0.4788 - val_accuracy: 0.8623\n",
      "Epoch 78/100\n",
      "24000/24000 [==============================] - 7s 312us/step - loss: 0.3680 - accuracy: 0.8932 - val_loss: 0.4758 - val_accuracy: 0.8639\n",
      "Epoch 79/100\n",
      "24000/24000 [==============================] - 8s 314us/step - loss: 0.3655 - accuracy: 0.8939 - val_loss: 0.4756 - val_accuracy: 0.8638\n",
      "Epoch 80/100\n",
      "24000/24000 [==============================] - 8s 314us/step - loss: 0.3639 - accuracy: 0.8944 - val_loss: 0.4754 - val_accuracy: 0.8634\n",
      "Epoch 81/100\n",
      "24000/24000 [==============================] - 8s 314us/step - loss: 0.3626 - accuracy: 0.8948 - val_loss: 0.4719 - val_accuracy: 0.8645\n",
      "Epoch 82/100\n",
      "24000/24000 [==============================] - 7s 304us/step - loss: 0.3596 - accuracy: 0.8958 - val_loss: 0.4724 - val_accuracy: 0.8644\n",
      "Epoch 83/100\n",
      "24000/24000 [==============================] - 8s 319us/step - loss: 0.3574 - accuracy: 0.8963 - val_loss: 0.4691 - val_accuracy: 0.8653\n",
      "Epoch 84/100\n",
      "24000/24000 [==============================] - 8s 313us/step - loss: 0.3555 - accuracy: 0.8969 - val_loss: 0.4689 - val_accuracy: 0.8654\n",
      "Epoch 85/100\n",
      "24000/24000 [==============================] - 8s 313us/step - loss: 0.3539 - accuracy: 0.8973 - val_loss: 0.4673 - val_accuracy: 0.8656\n",
      "Epoch 86/100\n",
      "24000/24000 [==============================] - 8s 317us/step - loss: 0.3519 - accuracy: 0.8979 - val_loss: 0.4656 - val_accuracy: 0.8661\n",
      "Epoch 87/100\n",
      "24000/24000 [==============================] - 7s 312us/step - loss: 0.3494 - accuracy: 0.8986 - val_loss: 0.4671 - val_accuracy: 0.8656\n",
      "Epoch 88/100\n",
      "24000/24000 [==============================] - 8s 314us/step - loss: 0.3476 - accuracy: 0.8992 - val_loss: 0.4650 - val_accuracy: 0.8661\n",
      "Epoch 89/100\n",
      "24000/24000 [==============================] - 8s 316us/step - loss: 0.3458 - accuracy: 0.8996 - val_loss: 0.4635 - val_accuracy: 0.8665\n",
      "Epoch 90/100\n",
      "24000/24000 [==============================] - 8s 313us/step - loss: 0.3432 - accuracy: 0.9005 - val_loss: 0.4643 - val_accuracy: 0.8668\n",
      "Epoch 91/100\n",
      "24000/24000 [==============================] - 8s 316us/step - loss: 0.3420 - accuracy: 0.9008 - val_loss: 0.4614 - val_accuracy: 0.8670\n",
      "Epoch 92/100\n",
      "24000/24000 [==============================] - 8s 314us/step - loss: 0.3396 - accuracy: 0.9014 - val_loss: 0.4597 - val_accuracy: 0.8681\n",
      "Epoch 93/100\n",
      "24000/24000 [==============================] - 8s 313us/step - loss: 0.3381 - accuracy: 0.9020 - val_loss: 0.4589 - val_accuracy: 0.8683\n",
      "Epoch 94/100\n",
      "24000/24000 [==============================] - 8s 314us/step - loss: 0.3356 - accuracy: 0.9026 - val_loss: 0.4595 - val_accuracy: 0.8680\n",
      "Epoch 95/100\n",
      "24000/24000 [==============================] - 8s 313us/step - loss: 0.3349 - accuracy: 0.9026 - val_loss: 0.4598 - val_accuracy: 0.8679\n",
      "Epoch 96/100\n",
      "24000/24000 [==============================] - 8s 315us/step - loss: 0.3326 - accuracy: 0.9034 - val_loss: 0.4586 - val_accuracy: 0.8683\n",
      "Epoch 97/100\n",
      "24000/24000 [==============================] - 8s 314us/step - loss: 0.3312 - accuracy: 0.9038 - val_loss: 0.4569 - val_accuracy: 0.8695\n",
      "Epoch 98/100\n",
      "24000/24000 [==============================] - 8s 318us/step - loss: 0.3285 - accuracy: 0.9045 - val_loss: 0.4552 - val_accuracy: 0.8697\n",
      "Epoch 99/100\n",
      "24000/24000 [==============================] - 8s 320us/step - loss: 0.3268 - accuracy: 0.9049 - val_loss: 0.4559 - val_accuracy: 0.8693\n",
      "Epoch 100/100\n",
      "24000/24000 [==============================] - 8s 320us/step - loss: 0.3254 - accuracy: 0.9055 - val_loss: 0.4541 - val_accuracy: 0.8700\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Идите.\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Споть мне!\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Убади!\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Остановитесь!\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Идите.\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Я виглаю.\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Отна не настень!\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Улыбайтесь.\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Забим себя!\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите одна.\n",
      "\n",
      "-\n",
      "Input sentence: He ran.\n",
      "Decoded sentence: Он меня.\n",
      "\n",
      "-\n",
      "Input sentence: I know.\n",
      "Decoded sentence: Я знаю.\n",
      "\n",
      "-\n",
      "Input sentence: I paid.\n",
      "Decoded sentence: Я простала.\n",
      "\n",
      "-\n",
      "Input sentence: I'm up.\n",
      "Decoded sentence: Я довой.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Не видь меня не соранил?\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Почень полодно?\n",
      "\n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: Попроси Тома.\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Будьте одновый.\n",
      "\n",
      "-\n",
      "Input sentence: Burn it.\n",
      "Decoded sentence: Выбритесь.\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Пойдите.\n",
      "\n",
      "-\n",
      "Input sentence: Come on!\n",
      "Decoded sentence: Полодни!\n",
      "\n",
      "-\n",
      "Input sentence: Fold it.\n",
      "Decoded sentence: Сложите его.\n",
      "\n",
      "-\n",
      "Input sentence: Get out!\n",
      "Decoded sentence: Провалайся!\n",
      "\n",
      "-\n",
      "Input sentence: Go away.\n",
      "Decoded sentence: Подожди ещё.\n",
      "\n",
      "-\n",
      "Input sentence: Go home.\n",
      "Decoded sentence: Иди за моня.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Help me!\n",
      "Decoded sentence: Помоги мне!\n",
      "\n",
      "-\n",
      "Input sentence: Hit Tom.\n",
      "Decoded sentence: Подмишь Том.\n",
      "\n",
      "-\n",
      "Input sentence: I agree.\n",
      "Decoded sentence: Я полодный.\n",
      "\n",
      "-\n",
      "Input sentence: I slept.\n",
      "Decoded sentence: Я потал.\n",
      "\n",
      "-\n",
      "Input sentence: I'll go.\n",
      "Decoded sentence: Я пойду.\n",
      "\n",
      "-\n",
      "Input sentence: I'm ill.\n",
      "Decoded sentence: Я володан.\n",
      "\n",
      "-\n",
      "Input sentence: I'm shy.\n",
      "Decoded sentence: Я том.\n",
      "\n",
      "-\n",
      "Input sentence: I'm wet.\n",
      "Decoded sentence: Я потредная.\n",
      "\n",
      "-\n",
      "Input sentence: Keep it.\n",
      "Decoded sentence: Оставьте его.\n",
      "\n",
      "-\n",
      "Input sentence: Lock it.\n",
      "Decoded sentence: Заприте его.\n",
      "\n",
      "-\n",
      "Input sentence: Perfect!\n",
      "Decoded sentence: Придеми!\n",
      "\n",
      "-\n",
      "Input sentence: Show me.\n",
      "Decoded sentence: Покажите мне.\n",
      "\n",
      "-\n",
      "Input sentence: Stop it.\n",
      "Decoded sentence: Прекратите его.\n",
      "\n",
      "-\n",
      "Input sentence: Tom won.\n",
      "Decoded sentence: Том был пол.\n",
      "\n",
      "-\n",
      "Input sentence: Wake up.\n",
      "Decoded sentence: Просысь.\n",
      "\n",
      "-\n",
      "Input sentence: Who won?\n",
      "Decoded sentence: Кто это слашил?\n",
      "\n",
      "-\n",
      "Input sentence: Am I fat?\n",
      "Decoded sentence: Я тобе полодно?\n",
      "\n",
      "-\n",
      "Input sentence: Back off.\n",
      "Decoded sentence: Отобрась.\n",
      "\n",
      "-\n",
      "Input sentence: Be brave.\n",
      "Decoded sentence: Будь такой.\n",
      "\n",
      "-\n",
      "Input sentence: Call Tom.\n",
      "Decoded sentence: Позови Тома.\n",
      "\n",
      "-\n",
      "Input sentence: Don't go.\n",
      "Decoded sentence: Не приси.\n",
      "\n",
      "-\n",
      "Input sentence: Fix this.\n",
      "Decoded sentence: Почините это.\n",
      "\n",
      "-\n",
      "Input sentence: Get down.\n",
      "Decoded sentence: Выбрайся.\n",
      "\n",
      "-\n",
      "Input sentence: Get lost!\n",
      "Decoded sentence: Пошлай покой!\n",
      "\n",
      "-\n",
      "Input sentence: Get real.\n",
      "Decoded sentence: Возвайтесь.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "# Подача выхода энкодера в декодер\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Подача выхода декодера в модель\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(0, 500, 10):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поднимемся на уровень слов, чтобы можно было что-то более адекватное посчитать за адекватное время"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Добавление Attention уровня"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "data_path = 'data/rus-eng/rus.txt'\n",
    "num_samples = 20000\n",
    "\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    \n",
    "    \"\"\"Предобработка пунктуации последовательности\"\"\"\n",
    "        \n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Zа-яА-Я]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(preprocess_sentence(input_text))\n",
    "    target_texts.append(preprocess_sentence(target_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<start> Go <end>', '<start> Go <end>'],\n",
       " ['<start> Марш <end>', '<start> Иди <end>'])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts[:2], target_texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go.\\tМарш!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1159202 (shanghainese)',\n",
       " 'Go.\\tИди.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #5898247 (marafon)',\n",
       " 'Go.\\tИдите.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #5898250 (marafon)',\n",
       " 'Hi.\\tЗдравствуйте.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #402127 (odexed)',\n",
       " 'Hi.\\tПривет!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #466968 (katjka)']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: Go.\n",
      "groundtruth: Марш!\n",
      "- CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1159202 (shanghainese)\n"
     ]
    }
   ],
   "source": [
    "inp, truth, _ = lines[0].split(\"\\t\")\n",
    "print(\"input:\", inp)\n",
    "print(\"groundtruth:\", truth)\n",
    "print(\"-\", _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> Go <end>'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_sentence(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> Марш <end>'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_sentence(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang:list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Токенизация текста\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    lang:list\n",
    "        Список последовательностей - выражений для одного языка\n",
    "        Пример input ['<start> Go . <end>', '<start> Go . <end>', ...]\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    tensor: numpy.ndarray\n",
    "        Тензор токенизированной последовательности\n",
    "        \n",
    "    lang_tokenizer: keras_preprocessing.text.Tokenizer\n",
    "            Токенайзер из модуля tf.keras\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, inp_lang_tokenizer = tokenize(input_texts)\n",
    "target_tensor, targ_lang_tokenizer = tokenize(target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = batch_size\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n",
    "vocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.lstm(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "    \n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.lstm(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 2.6567\n",
      "Epoch 2 Loss 2.0842\n",
      "Epoch 3 Loss 1.9482\n",
      "Epoch 4 Loss 1.8286\n",
      "Epoch 5 Loss 1.7521\n",
      "Epoch 6 Loss 1.7036\n",
      "Epoch 7 Loss 1.6659\n",
      "Epoch 8 Loss 1.6301\n",
      "Epoch 9 Loss 1.5914\n",
      "Epoch 10 Loss 1.5727\n",
      "Epoch 11 Loss 1.5124\n",
      "Epoch 12 Loss 1.4423\n",
      "Epoch 13 Loss 1.3724\n",
      "Epoch 14 Loss 1.3111\n",
      "Epoch 15 Loss 1.2599\n",
      "Epoch 16 Loss 1.2085\n",
      "Epoch 17 Loss 1.1609\n",
      "Epoch 18 Loss 1.1204\n",
      "Epoch 19 Loss 1.0795\n",
      "Epoch 20 Loss 1.0513\n",
      "Epoch 21 Loss 1.0098\n",
      "Epoch 22 Loss 0.9724\n",
      "Epoch 23 Loss 0.9350\n",
      "Epoch 24 Loss 0.9076\n",
      "Epoch 25 Loss 0.8749\n",
      "Epoch 26 Loss 0.8370\n",
      "Epoch 27 Loss 0.8033\n",
      "Epoch 28 Loss 0.7704\n",
      "Epoch 29 Loss 0.7424\n",
      "Epoch 30 Loss 0.7125\n",
      "Epoch 31 Loss 0.6824\n",
      "Epoch 32 Loss 0.6493\n",
      "Epoch 33 Loss 0.6157\n",
      "Epoch 34 Loss 0.5854\n",
      "Epoch 35 Loss 0.5566\n",
      "Epoch 36 Loss 0.5293\n",
      "Epoch 37 Loss 0.5037\n",
      "Epoch 38 Loss 0.4757\n",
      "Epoch 39 Loss 0.4509\n",
      "Epoch 40 Loss 0.4258\n",
      "Epoch 41 Loss 0.4088\n",
      "Epoch 42 Loss 0.3835\n",
      "Epoch 43 Loss 0.3619\n",
      "Epoch 44 Loss 0.3439\n",
      "Epoch 45 Loss 0.3260\n",
      "Epoch 46 Loss 0.3072\n",
      "Epoch 47 Loss 0.2914\n",
      "Epoch 48 Loss 0.2771\n",
      "Epoch 49 Loss 0.2648\n",
      "Epoch 50 Loss 0.2529\n",
      "Epoch 51 Loss 0.2411\n",
      "Epoch 52 Loss 0.2305\n",
      "Epoch 53 Loss 0.2205\n",
      "Epoch 54 Loss 0.2130\n",
      "Epoch 55 Loss 0.2058\n",
      "Epoch 56 Loss 0.1982\n",
      "Epoch 57 Loss 0.1909\n",
      "Epoch 58 Loss 0.1861\n",
      "Epoch 59 Loss 0.1786\n",
      "Epoch 60 Loss 0.1732\n",
      "Epoch 61 Loss 0.1694\n",
      "Epoch 62 Loss 0.1655\n",
      "Epoch 63 Loss 0.1633\n",
      "Epoch 64 Loss 0.1593\n",
      "Epoch 65 Loss 0.1537\n",
      "Epoch 66 Loss 0.1511\n",
      "Epoch 67 Loss 0.1486\n",
      "Epoch 68 Loss 0.1456\n",
      "Epoch 69 Loss 0.1430\n",
      "Epoch 70 Loss 0.1388\n",
      "Epoch 71 Loss 0.1371\n",
      "Epoch 72 Loss 0.1357\n",
      "Epoch 73 Loss 0.1331\n",
      "Epoch 74 Loss 0.1312\n",
      "Epoch 75 Loss 0.1306\n",
      "Epoch 76 Loss 0.1286\n",
      "Epoch 77 Loss 0.1263\n",
      "Epoch 78 Loss 0.1244\n",
      "Epoch 79 Loss 0.1221\n",
      "Epoch 80 Loss 0.1216\n",
      "Epoch 81 Loss 0.1213\n",
      "Epoch 82 Loss 0.1200\n",
      "Epoch 83 Loss 0.1197\n",
      "Epoch 84 Loss 0.1176\n",
      "Epoch 85 Loss 0.1174\n",
      "Epoch 86 Loss 0.1169\n",
      "Epoch 87 Loss 0.1150\n",
      "Epoch 88 Loss 0.1167\n",
      "Epoch 89 Loss 0.1164\n",
      "Epoch 90 Loss 0.1155\n",
      "Epoch 91 Loss 0.1136\n",
      "Epoch 92 Loss 0.1125\n",
      "Epoch 93 Loss 0.1123\n",
      "Epoch 94 Loss 0.1102\n",
      "Epoch 95 Loss 0.1101\n",
      "Epoch 96 Loss 0.1102\n",
      "Epoch 97 Loss 0.1090\n",
      "Epoch 98 Loss 0.1076\n",
      "Epoch 99 Loss 0.1068\n",
      "Epoch 100 Loss 0.1067\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker\n",
    "\n",
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    fontdict = {'fontsize': 14}\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    plt.show()\n",
    "    \n",
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nickel\\anaconda3\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['f']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> good morning <end>\n",
      "Predicted translation: доброе утро <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "translate(u'good morning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Input: <start> good for you <end>\n",
      "Predicted translation: рад за тебя <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "translate(u'good for you')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея в том, что каждое слово параллельно проходит через слои, изображенные на картинке. Некоторые из них — это стандартные fully-connected layers, некоторые — shortcut connections как в ResNet (там, где на картинке Add).\n",
    "\n",
    "Multi-head attention - это специальный новый слой, который дает возможность каждому входному вектору взаимодействовать с другими словами через attention mechanism, вместо передачи hidden state как в RNN или соседних слов как в CNN.\n",
    "\n",
    "Работа энкодера:\n",
    "\n",
    "Делаются эмбеддинги для всех слов предложения (вектора одинаковой размерности). Для примера пусть это будет предложение I am stupid. В эмбеддинг добавляется еще позиция слова в предложении.\n",
    "\n",
    "Берется вектор первого слова и вектор второго слова (I, am), подаются на однослойную сеть с одним выходом, которая выдает степень их похожести (скалярная величина). Эта скалярная величина умножается на вектор второго слова, получая его некоторую \"ослабленную\" на величину похожести копию.\n",
    "\n",
    "Вместо второго слова подается третье слово и делается тоже самое что в п.2. с той же самой сетью с теми же весами (для векторов I, stupid).\n",
    "\n",
    "Делая тоже самое для всех оставшихся слов предложения получаются их \"ослабленные\" (взвешенные) копии, которые выражают степень их похожести на первое слово. Далее эти все взвешенные вектора складываются друг с другом, получая один результирующий вектор размерности одного эмбединга: output=am * weight(I, am) + stupid * weight(I, stupid)\n",
    "\n",
    "Это механизм \"обычного\" attention. Так как оценка похожести слов всего одним способом (по одному критерию) считается недостаточной, тоже самое (п.2-4) повторяется несколько раз с другими весами. Типа одна один attention может определять похожесть слов по смысловой нагрузке, другой по грамматической, остальные еще как-то и т.п.\n",
    "\n",
    "На выходе п.5. получается несколько векторов, каждый из которых является взвешенной суммой всех остальных слов предложения относительно их похожести на первое слово (I). Конкантенируем этот вректор в один.\n",
    "\n",
    "Дальше ставится еще один слой линейного преобразования, уменьшающий размерность результата п.6. до размерности вектора одного эмбединга. Получается некое представление первого слова предложения, составленное из взвешенных векторов всех остальных слов предложения.\n",
    "\n",
    "Такой же процесс производится для всех других слов в предложении.\n",
    "\n",
    "Так как размерность выхода та же, то можно проделать все тоже самое еще раз (п.2-8), но вместо оригинальных эмбеддингов слов взять то, что получается после прохода через этот Multi-head attention, а нейросети аттеншенов внутри взять с другими весами (веса между слоями не общие). И таких слоев можно сделать много (у гугла 6). Однако между первым и вторым слоем добавляется еще полносвязный слой и residual соединения, чтобы добавить сети выразительности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \n",
    "        \"\"\"Разделение последней размерности на (num_heads, depth).\n",
    "        Транспонирование реультата к размерности (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "    \n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n",
    "    \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "    \n",
    "    \n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = len(inp_lang_tokenizer.index_word) + 2\n",
    "target_vocab_size = len(targ_lang_tokenizer.index_word) + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "  \n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')\n",
    "\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "  \n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 8.8796 Accuracy 0.0000\n",
      "Epoch 1 Loss 8.8696 Accuracy 0.0000\n",
      "Epoch 2 Batch 0 Loss 8.8444 Accuracy 0.0001\n",
      "Epoch 2 Loss 8.7998 Accuracy 0.0081\n",
      "Epoch 3 Batch 0 Loss 8.7371 Accuracy 0.0562\n",
      "Epoch 3 Loss 8.6710 Accuracy 0.0996\n",
      "Epoch 4 Batch 0 Loss 8.5908 Accuracy 0.1111\n",
      "Epoch 4 Loss 8.5311 Accuracy 0.1111\n",
      "Epoch 5 Batch 0 Loss 8.4604 Accuracy 0.1111\n",
      "Epoch 5 Loss 8.4066 Accuracy 0.1111\n",
      "Epoch 6 Batch 0 Loss 8.3443 Accuracy 0.1111\n",
      "Epoch 6 Loss 8.2982 Accuracy 0.1111\n",
      "Epoch 7 Batch 0 Loss 8.2458 Accuracy 0.1111\n",
      "Epoch 7 Loss 8.2029 Accuracy 0.1111\n",
      "Epoch 8 Batch 0 Loss 8.1556 Accuracy 0.1111\n",
      "Epoch 8 Loss 8.1095 Accuracy 0.1111\n",
      "Epoch 9 Batch 0 Loss 8.0640 Accuracy 0.1111\n",
      "Epoch 9 Loss 8.0102 Accuracy 0.1111\n",
      "Epoch 10 Batch 0 Loss 7.9506 Accuracy 0.1111\n",
      "Epoch 10 Loss 7.8959 Accuracy 0.1111\n",
      "Epoch 11 Batch 0 Loss 7.8158 Accuracy 0.1111\n",
      "Epoch 11 Loss 7.7670 Accuracy 0.1111\n",
      "Epoch 12 Batch 0 Loss 7.6890 Accuracy 0.1111\n",
      "Epoch 12 Loss 7.6301 Accuracy 0.1126\n",
      "Epoch 13 Batch 0 Loss 7.5651 Accuracy 0.1186\n",
      "Epoch 13 Loss 7.4883 Accuracy 0.1281\n",
      "Epoch 14 Batch 0 Loss 7.4207 Accuracy 0.1343\n",
      "Epoch 14 Loss 7.3420 Accuracy 0.1357\n",
      "Epoch 15 Batch 0 Loss 7.2586 Accuracy 0.1351\n",
      "Epoch 15 Loss 7.1897 Accuracy 0.1366\n",
      "Epoch 16 Batch 0 Loss 7.1032 Accuracy 0.1354\n",
      "Epoch 16 Loss 7.0333 Accuracy 0.1368\n",
      "Epoch 17 Batch 0 Loss 6.9646 Accuracy 0.1348\n",
      "Epoch 17 Loss 6.8732 Accuracy 0.1370\n",
      "Epoch 18 Batch 0 Loss 6.7882 Accuracy 0.1363\n",
      "Epoch 18 Loss 6.7111 Accuracy 0.1377\n",
      "Epoch 19 Batch 0 Loss 6.6188 Accuracy 0.1400\n",
      "Epoch 19 Loss 6.5466 Accuracy 0.1409\n",
      "Epoch 20 Batch 0 Loss 6.4603 Accuracy 0.1436\n",
      "Epoch 20 Loss 6.3826 Accuracy 0.1443\n",
      "Epoch 21 Batch 0 Loss 6.2904 Accuracy 0.1454\n",
      "Epoch 21 Loss 6.2214 Accuracy 0.1463\n",
      "Epoch 22 Batch 0 Loss 6.1425 Accuracy 0.1469\n",
      "Epoch 22 Loss 6.0607 Accuracy 0.1474\n",
      "Epoch 23 Batch 0 Loss 5.9709 Accuracy 0.1466\n",
      "Epoch 23 Loss 5.9055 Accuracy 0.1475\n",
      "Epoch 24 Batch 0 Loss 5.8366 Accuracy 0.1478\n",
      "Epoch 24 Loss 5.7533 Accuracy 0.1484\n",
      "Epoch 25 Batch 0 Loss 5.6544 Accuracy 0.1477\n",
      "Epoch 25 Loss 5.6060 Accuracy 0.1487\n",
      "Epoch 26 Batch 0 Loss 5.5335 Accuracy 0.1469\n",
      "Epoch 26 Loss 5.4662 Accuracy 0.1488\n",
      "Epoch 27 Batch 0 Loss 5.3781 Accuracy 0.1493\n",
      "Epoch 27 Loss 5.3287 Accuracy 0.1492\n",
      "Epoch 28 Batch 0 Loss 5.2521 Accuracy 0.1509\n",
      "Epoch 28 Loss 5.1918 Accuracy 0.1500\n",
      "Epoch 29 Batch 0 Loss 5.1248 Accuracy 0.1512\n",
      "Epoch 29 Loss 5.0605 Accuracy 0.1519\n",
      "Epoch 30 Batch 0 Loss 4.9485 Accuracy 0.1552\n",
      "Epoch 30 Loss 4.9274 Accuracy 0.1544\n",
      "Epoch 31 Batch 0 Loss 4.8936 Accuracy 0.1515\n",
      "Epoch 31 Loss 4.7950 Accuracy 0.1579\n",
      "Epoch 32 Batch 0 Loss 4.7075 Accuracy 0.1580\n",
      "Epoch 32 Loss 4.6721 Accuracy 0.1621\n",
      "Epoch 33 Batch 0 Loss 4.5810 Accuracy 0.1632\n",
      "Epoch 33 Loss 4.5508 Accuracy 0.1676\n",
      "Epoch 34 Batch 0 Loss 4.4750 Accuracy 0.1698\n",
      "Epoch 34 Loss 4.4376 Accuracy 0.1721\n",
      "Epoch 35 Batch 0 Loss 4.3568 Accuracy 0.1753\n",
      "Epoch 35 Loss 4.3285 Accuracy 0.1759\n",
      "Epoch 36 Batch 0 Loss 4.2448 Accuracy 0.1780\n",
      "Epoch 36 Loss 4.2287 Accuracy 0.1785\n",
      "Epoch 37 Batch 0 Loss 4.1887 Accuracy 0.1793\n",
      "Epoch 37 Loss 4.1336 Accuracy 0.1814\n",
      "Epoch 38 Batch 0 Loss 4.0916 Accuracy 0.1838\n",
      "Epoch 38 Loss 4.0515 Accuracy 0.1838\n",
      "Epoch 39 Batch 0 Loss 4.0126 Accuracy 0.1850\n",
      "Epoch 39 Loss 3.9606 Accuracy 0.1866\n",
      "Epoch 40 Batch 0 Loss 3.9025 Accuracy 0.1883\n",
      "Epoch 40 Loss 3.8845 Accuracy 0.1884\n",
      "Epoch 41 Batch 0 Loss 3.8525 Accuracy 0.1876\n",
      "Epoch 41 Loss 3.8081 Accuracy 0.1908\n",
      "Epoch 42 Batch 0 Loss 3.8193 Accuracy 0.1918\n",
      "Epoch 42 Loss 3.7349 Accuracy 0.1928\n",
      "Epoch 43 Batch 0 Loss 3.6673 Accuracy 0.1923\n",
      "Epoch 43 Loss 3.6662 Accuracy 0.1946\n",
      "Epoch 44 Batch 0 Loss 3.6019 Accuracy 0.1976\n",
      "Epoch 44 Loss 3.5959 Accuracy 0.1964\n",
      "Epoch 45 Batch 0 Loss 3.5616 Accuracy 0.1955\n",
      "Epoch 45 Loss 3.5318 Accuracy 0.1984\n",
      "Epoch 46 Batch 0 Loss 3.5110 Accuracy 0.1995\n",
      "Epoch 46 Loss 3.4741 Accuracy 0.1998\n",
      "Epoch 47 Batch 0 Loss 3.4476 Accuracy 0.1995\n",
      "Epoch 47 Loss 3.4154 Accuracy 0.2011\n",
      "Epoch 48 Batch 0 Loss 3.3416 Accuracy 0.2045\n",
      "Epoch 48 Loss 3.3502 Accuracy 0.2030\n",
      "Epoch 49 Batch 0 Loss 3.3339 Accuracy 0.2030\n",
      "Epoch 49 Loss 3.2898 Accuracy 0.2051\n",
      "Epoch 50 Batch 0 Loss 3.1860 Accuracy 0.2108\n",
      "Epoch 50 Loss 3.2323 Accuracy 0.2066\n",
      "Epoch 51 Batch 0 Loss 3.2226 Accuracy 0.2045\n",
      "Epoch 51 Loss 3.1794 Accuracy 0.2089\n",
      "Epoch 52 Batch 0 Loss 3.2006 Accuracy 0.2061\n",
      "Epoch 52 Loss 3.1217 Accuracy 0.2104\n",
      "Epoch 53 Batch 0 Loss 3.0998 Accuracy 0.2110\n",
      "Epoch 53 Loss 3.0636 Accuracy 0.2130\n",
      "Epoch 54 Batch 0 Loss 3.0535 Accuracy 0.2147\n",
      "Epoch 54 Loss 3.0094 Accuracy 0.2150\n",
      "Epoch 55 Batch 0 Loss 3.0382 Accuracy 0.2139\n",
      "Epoch 55 Loss 2.9526 Accuracy 0.2169\n",
      "Epoch 56 Batch 0 Loss 2.9122 Accuracy 0.2172\n",
      "Epoch 56 Loss 2.9052 Accuracy 0.2191\n",
      "Epoch 57 Batch 0 Loss 2.8817 Accuracy 0.2189\n",
      "Epoch 57 Loss 2.8412 Accuracy 0.2212\n",
      "Epoch 58 Batch 0 Loss 2.8297 Accuracy 0.2231\n",
      "Epoch 58 Loss 2.7920 Accuracy 0.2233\n",
      "Epoch 59 Batch 0 Loss 2.7409 Accuracy 0.2233\n",
      "Epoch 59 Loss 2.7355 Accuracy 0.2253\n",
      "Epoch 60 Batch 0 Loss 2.6883 Accuracy 0.2270\n",
      "Epoch 60 Loss 2.6873 Accuracy 0.2278\n",
      "Epoch 61 Batch 0 Loss 2.6639 Accuracy 0.2268\n",
      "Epoch 61 Loss 2.6337 Accuracy 0.2300\n",
      "Epoch 62 Batch 0 Loss 2.5977 Accuracy 0.2306\n",
      "Epoch 62 Loss 2.5809 Accuracy 0.2326\n",
      "Epoch 63 Batch 0 Loss 2.6016 Accuracy 0.2335\n",
      "Epoch 63 Loss 2.5327 Accuracy 0.2339\n",
      "Epoch 64 Batch 0 Loss 2.4794 Accuracy 0.2367\n",
      "Epoch 64 Loss 2.4831 Accuracy 0.2361\n",
      "Epoch 65 Batch 0 Loss 2.4752 Accuracy 0.2348\n",
      "Epoch 65 Loss 2.4338 Accuracy 0.2375\n",
      "Epoch 66 Batch 0 Loss 2.3959 Accuracy 0.2412\n",
      "Epoch 66 Loss 2.3830 Accuracy 0.2400\n",
      "Epoch 67 Batch 0 Loss 2.3080 Accuracy 0.2447\n",
      "Epoch 67 Loss 2.3332 Accuracy 0.2421\n",
      "Epoch 68 Batch 0 Loss 2.2805 Accuracy 0.2409\n",
      "Epoch 68 Loss 2.2854 Accuracy 0.2444\n",
      "Epoch 69 Batch 0 Loss 2.2631 Accuracy 0.2461\n",
      "Epoch 69 Loss 2.2398 Accuracy 0.2465\n",
      "Epoch 70 Batch 0 Loss 2.1727 Accuracy 0.2538\n",
      "Epoch 70 Loss 2.1900 Accuracy 0.2487\n",
      "Epoch 71 Batch 0 Loss 2.1276 Accuracy 0.2505\n",
      "Epoch 71 Loss 2.1346 Accuracy 0.2517\n",
      "Epoch 72 Batch 0 Loss 2.1072 Accuracy 0.2526\n",
      "Epoch 72 Loss 2.0924 Accuracy 0.2537\n",
      "Epoch 73 Batch 0 Loss 2.0324 Accuracy 0.2578\n",
      "Epoch 73 Loss 2.0503 Accuracy 0.2556\n",
      "Epoch 74 Batch 0 Loss 2.0323 Accuracy 0.2556\n",
      "Epoch 74 Loss 2.0045 Accuracy 0.2574\n",
      "Epoch 75 Batch 0 Loss 1.9680 Accuracy 0.2595\n",
      "Epoch 75 Loss 1.9561 Accuracy 0.2600\n",
      "Epoch 76 Batch 0 Loss 1.9485 Accuracy 0.2673\n",
      "Epoch 76 Loss 1.9101 Accuracy 0.2625\n",
      "Epoch 77 Batch 0 Loss 1.8802 Accuracy 0.2700\n",
      "Epoch 77 Loss 1.8667 Accuracy 0.2647\n",
      "Epoch 78 Batch 0 Loss 1.7983 Accuracy 0.2712\n",
      "Epoch 78 Loss 1.8230 Accuracy 0.2675\n",
      "Epoch 79 Batch 0 Loss 1.8808 Accuracy 0.2625\n",
      "Epoch 79 Loss 1.7864 Accuracy 0.2691\n",
      "Epoch 80 Batch 0 Loss 1.7371 Accuracy 0.2725\n",
      "Epoch 80 Loss 1.7344 Accuracy 0.2724\n",
      "Epoch 81 Batch 0 Loss 1.7390 Accuracy 0.2720\n",
      "Epoch 81 Loss 1.6946 Accuracy 0.2745\n",
      "Epoch 82 Batch 0 Loss 1.6177 Accuracy 0.2790\n",
      "Epoch 82 Loss 1.6614 Accuracy 0.2764\n",
      "Epoch 83 Batch 0 Loss 1.5906 Accuracy 0.2812\n",
      "Epoch 83 Loss 1.6151 Accuracy 0.2788\n",
      "Epoch 84 Batch 0 Loss 1.5527 Accuracy 0.2815\n",
      "Epoch 84 Loss 1.5702 Accuracy 0.2823\n",
      "Epoch 85 Batch 0 Loss 1.4981 Accuracy 0.2853\n",
      "Epoch 85 Loss 1.5274 Accuracy 0.2843\n",
      "Epoch 86 Batch 0 Loss 1.4813 Accuracy 0.2952\n",
      "Epoch 86 Loss 1.4899 Accuracy 0.2873\n",
      "Epoch 87 Batch 0 Loss 1.4628 Accuracy 0.2909\n",
      "Epoch 87 Loss 1.4472 Accuracy 0.2902\n",
      "Epoch 88 Batch 0 Loss 1.4012 Accuracy 0.2923\n",
      "Epoch 88 Loss 1.4125 Accuracy 0.2917\n",
      "Epoch 89 Batch 0 Loss 1.3702 Accuracy 0.2930\n",
      "Epoch 89 Loss 1.3673 Accuracy 0.2949\n",
      "Epoch 90 Batch 0 Loss 1.3274 Accuracy 0.3026\n",
      "Epoch 90 Loss 1.3345 Accuracy 0.2974\n",
      "Epoch 91 Batch 0 Loss 1.2929 Accuracy 0.3001\n",
      "Epoch 91 Loss 1.2969 Accuracy 0.2987\n",
      "Epoch 92 Batch 0 Loss 1.2567 Accuracy 0.3023\n",
      "Epoch 92 Loss 1.2608 Accuracy 0.3019\n",
      "Epoch 93 Batch 0 Loss 1.2411 Accuracy 0.3031\n",
      "Epoch 93 Loss 1.2229 Accuracy 0.3043\n",
      "Epoch 94 Batch 0 Loss 1.2145 Accuracy 0.3022\n",
      "Epoch 94 Loss 1.1909 Accuracy 0.3066\n",
      "Epoch 95 Batch 0 Loss 1.1497 Accuracy 0.3103\n",
      "Epoch 95 Loss 1.1486 Accuracy 0.3091\n",
      "Epoch 96 Batch 0 Loss 1.1109 Accuracy 0.3164\n",
      "Epoch 96 Loss 1.1202 Accuracy 0.3116\n",
      "Epoch 97 Batch 0 Loss 1.0648 Accuracy 0.3173\n",
      "Epoch 97 Loss 1.0859 Accuracy 0.3138\n",
      "Epoch 98 Batch 0 Loss 1.0533 Accuracy 0.3185\n",
      "Epoch 98 Loss 1.0565 Accuracy 0.3151\n",
      "Epoch 99 Batch 0 Loss 1.0485 Accuracy 0.3169\n",
      "Epoch 99 Loss 1.0255 Accuracy 0.3167\n",
      "Epoch 100 Batch 0 Loss 0.9907 Accuracy 0.3188\n",
      "Epoch 100 Loss 0.9953 Accuracy 0.3193\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "for epoch in range(100):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "  \n",
    "    for (batch, (inp, tar)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        train_step(inp, tar)\n",
    "        if batch % 50 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    start_token = [1]\n",
    "    end_token = [2]\n",
    "  \n",
    "    sentence = preprocess_sentence(inp_sentence)\n",
    "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    \n",
    "    encoder_input = tf.expand_dims(inputs, 0)\n",
    "  \n",
    "  # as the target is english, the first word to the transformer should be the\n",
    "  # english start token.\n",
    "    decoder_input = [1]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(max_length_targ):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "  \n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    \n",
    "    # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "    # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == targ_lang_tokenizer.word_index[\"<end>\"]:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "    # concatentate the predicted_id to the output which is given to the decoder\n",
    "    # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "\n",
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "  \n",
    "    sentence = inp_lang_tokenizer.encode(sentence)\n",
    "  \n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    "  \n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    "\n",
    "        # plot the attention weights\n",
    "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        ax.set_xticks(range(len(sentence)+2))\n",
    "        ax.set_yticks(range(len(result)))\n",
    "\n",
    "        ax.set_ylim(len(result)-1.5, -0.5)\n",
    "\n",
    "        ax.set_xticklabels(\n",
    "            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "            fontdict=fontdict, rotation=90)\n",
    "\n",
    "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "                            if i < tokenizer_en.vocab_size], \n",
    "                           fontdict=fontdict)\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    "  \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def translate(sentence, plot=''):\n",
    "    result, attention_weights = evaluate(sentence)\n",
    "    predicted_sentence = ([targ_lang_tokenizer.index_word[i] for i in result.numpy()])  \n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(predicted_sentence))\n",
    "  \n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sentence, result, plot)\n",
    "        \n",
    "# translate(\"good morning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: wake up and drink\n",
      "Predicted translation: ['<start>', 'сходи', 'выше']\n"
     ]
    }
   ],
   "source": [
    "translate('wake up and drink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: get up and drink\n",
      "Predicted translation: ['<start>', 'выпей', 'немного']\n"
     ]
    }
   ],
   "source": [
    "translate('get up and drink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: what do you mean\n",
      "Predicted translation: ['<start>', 'что', 'ты', 'будешь', 'счастливой']\n"
     ]
    }
   ],
   "source": [
    "translate(\"what do you mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: good morning.\n",
      "Predicted translation: ['<start>', 'с', 'добрым', 'утром']\n"
     ]
    }
   ],
   "source": [
    "translate(\"good morning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: hi, how are you?\n",
      "Predicted translation: ['<start>', 'как', 'дела']\n"
     ]
    }
   ],
   "source": [
    "translate('hi, how are you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: no why do not\n",
      "Predicted translation: ['<start>', 'почему', 'нет']\n"
     ]
    }
   ],
   "source": [
    "translate('no why do not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: get over here\n",
      "Predicted translation: ['<start>', 'иди', 'сюда']\n"
     ]
    }
   ],
   "source": [
    "translate('get over here')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "\n",
    "- Русский модуль достаточно сложный для Attention Transformer - как самостоятельную модель его использовать не получится.\n",
    "- Учебные словари в примерах весьма скудны, там отсутствует большое количество слов, что также сказывается на переводе.\n",
    "- Генерация char-последовательностей (на уровне символов) увеличиает долю ошибок (особенно с окончаниями), при должном уровне обучения его можно локально применять для генерации новых слов. [мой пример обучения более длинной последовательности lstm  ~4 суток на проце i3 на чистом numpy (и этого тоже недостаточного)](https://github.com/Nickel-nc/GU_Introduction_To_Neural_Networks/blob/master/LSTM%20models/Text_gen_experiment.ipynb)\n",
    "- Возможно снижение количества ошибочных предсказаний при использовании идеи Seq2Seq, в связке с более легковесными моделями, работающими со стандартизированными словарями, где все слова имеют нормальную морфологическую форму, с последующей передачей предсказания в генератор для придания тексту дополнительного стилистического окраса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
