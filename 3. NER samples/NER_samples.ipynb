{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset: tweeter messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# Import libs\n",
    "##############\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import RegexpTagger\n",
    "from collections import Counter\n",
    "import html\n",
    "from utils.dicts import apostrophe_dict, emoticon_dict, short_word_dict\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Nickel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Nickel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Nickel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# Settings\n",
    "###########\n",
    "\n",
    "TOP_LIMIT = 20\n",
    "\n",
    "# Data load\n",
    "with open('data/df_processed.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "    \n",
    "train_df = pd.read_csv('data/train_tweets.csv')\n",
    "test_df = pd.read_csv('data/test_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>when father is dysfunctional and is so selfish...</td>\n",
       "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
       "      <td>[father, dysfunctional, selfish, drags, kids, ...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks for lyft credit cannot use cause they d...</td>\n",
       "      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "      <td>[bihday, majesti]</td>\n",
       "      <td>[bihday, majesti]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>model love you take with you all the time in ur</td>\n",
       "      <td>[model, love, you, take, with, you, all, the, ...</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society now motivation</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "      <td>[factsguide, society, motivation]</td>\n",
       "      <td>[factsguid, societi, motiv]</td>\n",
       "      <td>[factsguid, societi, motiv]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0   @user when a father is dysfunctional and is s...   \n",
       "1   2    0.0  @user @user thanks for #lyft credit i can't us...   \n",
       "2   3    0.0                                bihday your majesty   \n",
       "3   4    0.0  #model   i love u take with u all the time in ...   \n",
       "4   5    0.0             factsguide: society now    #motivation   \n",
       "\n",
       "                                         clean_tweet  \\\n",
       "0  when father is dysfunctional and is so selfish...   \n",
       "1  thanks for lyft credit cannot use cause they d...   \n",
       "2                                bihday your majesty   \n",
       "3    model love you take with you all the time in ur   \n",
       "4                  factsguide society now motivation   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  [when, father, is, dysfunctional, and, is, so,...   \n",
       "1  [thanks, for, lyft, credit, can, not, use, cau...   \n",
       "2                            [bihday, your, majesty]   \n",
       "3  [model, love, you, take, with, you, all, the, ...   \n",
       "4             [factsguide, society, now, motivation]   \n",
       "\n",
       "                                tweet_token_filtered  \\\n",
       "0  [father, dysfunctional, selfish, drags, kids, ...   \n",
       "1  [thanks, lyft, credit, use, cause, offer, whee...   \n",
       "2                                  [bihday, majesty]   \n",
       "3                      [model, love, take, time, ur]   \n",
       "4                  [factsguide, society, motivation]   \n",
       "\n",
       "                                       tweet_stemmed  \\\n",
       "0  [father, dysfunct, selfish, drag, kid, dysfunc...   \n",
       "1  [thank, lyft, credit, use, caus, offer, wheelc...   \n",
       "2                                  [bihday, majesti]   \n",
       "3                      [model, love, take, time, ur]   \n",
       "4                        [factsguid, societi, motiv]   \n",
       "\n",
       "                                    tweet_lemmatized  \n",
       "0  [father, dysfunct, selfish, drag, kid, dysfunc...  \n",
       "1  [thank, lyft, credit, use, caus, offer, wheelc...  \n",
       "2                                  [bihday, majesti]  \n",
       "3                      [model, love, take, time, ur]  \n",
       "4                        [factsguid, societi, motiv]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Preprocess Routine\n",
    "######################\n",
    "###################\n",
    "\n",
    "\n",
    "def extract_text(df, target='clean_tweet'):\n",
    "    \n",
    "    \"\"\"Extract raw text from tweets Data Frame\"\"\"\n",
    "    \n",
    "    return \" \".join(df[target])\n",
    "    \n",
    "\n",
    "def replace_from_dict(text, source_dict):\n",
    "    \n",
    "    \"\"\"Search through text, map tokens via dict, place default if not in dict\"\"\"\n",
    "    \n",
    "    return \" \".join([source_dict.get(word, word) for word in text.split()])\n",
    "\n",
    "\n",
    "def remove_onechar_tokens(text):\n",
    "    \n",
    "    \"\"\"Search through text, remove one-caracter tokens\"\"\"\n",
    "    \n",
    "    return ' '.join([w for w in text.split() if len(w)>1])\n",
    "\n",
    "\n",
    "def filter_stop_words(tokens, stop_words=None):\n",
    "    \n",
    "    \"\"\"Remove stop words from tokens\"\"\"\n",
    "    \n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "\n",
    "def stem_tokens(tokens, stemmer=None):\n",
    "    \n",
    "    \"\"\"Stemming preprocessing\"\"\"\n",
    "    \n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "\n",
    "def lemmatize_tokens(tokens, lemmatizer=None):\n",
    "    \n",
    "    \"\"\"Lemmatize preprocessing\"\"\"\n",
    "    \n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "\n",
    "def preprocess(df,\n",
    "               src_col='tweet',\n",
    "               clean_col='clean_tweet',\n",
    "               token_col=None,\n",
    "               filter_col=None,\n",
    "               stemmed_col=None,\n",
    "               lem_col=None\n",
    "              ):\n",
    "     \n",
    "    \"\"\"\n",
    "    Modified preprocessor for NER testing.\n",
    "    \n",
    "    Changes: \n",
    "    - case processing str.lower() changed for str.upper() \n",
    "    - tokenization, stemming, lemmatization\n",
    "    \"\"\"\n",
    "        \n",
    "    # 1. Clean html context\n",
    "    df[clean_col] = df[src_col].apply(lambda x: html.unescape(x))\n",
    "    \n",
    "    # 2. Remove @user references\n",
    "    df[clean_col] = df[clean_col].apply(lambda x: re.sub(r'@[\\w]*','', x))\n",
    "    \n",
    "    # 3. Correct register to lowercase\n",
    "    df[clean_col] = df[clean_col].str.upper()\n",
    "    \n",
    "    # 4. Change apostrophes\n",
    "    vfunc = np.vectorize(replace_from_dict)\n",
    "    df[clean_col] = vfunc(df[clean_col], apostrophe_dict)\n",
    "    \n",
    "    # 5. Prolong short words\n",
    "    df[clean_col] = vfunc(df[clean_col], short_word_dict)\n",
    "    \n",
    "    # 6. Replace emoticons\n",
    "    df[clean_col] = vfunc(df[clean_col], emoticon_dict)\n",
    "    \n",
    "    # 7. Replace punctuation to spaces\n",
    "    df[clean_col] = df[clean_col].apply(lambda x: re.sub(r'[^\\w\\s]','', x))\n",
    "    \n",
    "    # 8. Replace special characters to spaces\n",
    "    df[clean_col] = df[clean_col].apply(lambda x: re.sub(r'[^a-zA-Z0-9]', ' ', x))\n",
    "    \n",
    "    # 9. Replace nums for spaces\n",
    "    df[clean_col] = df[clean_col].apply(lambda x: re.sub(r'[^a-zA-Z]', ' ', x))\n",
    "    \n",
    "    # 10. Drop one char words\n",
    "    vfunc = np.vectorize(remove_onechar_tokens)\n",
    "    df[clean_col] = vfunc(df[clean_col])\n",
    "    \n",
    "    # 11. Tokenize text\n",
    "    # df[token_col] = df[clean_col].apply(lambda x:  nltk.tokenize.word_tokenize(x))\n",
    "    \n",
    "    # 12. Filter stop words\n",
    "    # stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    # df[filter_col] = df[token_col].apply(lambda x: filter_stop_words(x))\n",
    "    \n",
    "    # 13. Apply stemming\n",
    "    # df[stemmed_col] = df[filter_col].apply(lambda x: stem_tokens(x))\n",
    "    \n",
    "    # 14. Lemmatize\n",
    "    # df[lem_col] = df[stemmed_col].apply(lambda x: lemmatize_tokens(x))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER via LNTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tags = nltk.pos_tag(nltk.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('when', 'WRB'),\n",
       " ('father', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('dysfunctional', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('is', 'VBZ'),\n",
       " ('so', 'RB'),\n",
       " ('selfish', 'JJ'),\n",
       " ('he', 'PRP'),\n",
       " ('drags', 'VBZ'),\n",
       " ('his', 'PRP$'),\n",
       " ('kids', 'NNS'),\n",
       " ('into', 'IN'),\n",
       " ('his', 'PRP$'),\n",
       " ('dysfunction', 'NN'),\n",
       " ('run', 'VB'),\n",
       " ('thanks', 'NNS'),\n",
       " ('for', 'IN'),\n",
       " ('lyft', 'JJ'),\n",
       " ('credit', 'NN')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_tags[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for named entities via  nltk.ne_chunk()\n",
    "\n",
    "named_entities = {(' '.join(c[0] for c in chunk), chunk.label() ) \n",
    "                  for chunk in nltk.ne_chunk(nltk_tags) \n",
    "                  if hasattr(chunk, 'label') }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Named entities not found\n",
    "named_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Try preprocessor with upper case correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate new data\n",
    "new_target = 'clean_tweet_uppercase'\n",
    "df = preprocess(df, clean_col=new_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>clean_tweet_uppercase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when father is dysfunctional and is so selfish...</td>\n",
       "      <td>WHEN FATHER IS DYSFUNCTIONAL AND IS SO SELFISH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thanks for lyft credit cannot use cause they d...</td>\n",
       "      <td>THANKS FOR LYFT CREDIT CANT USE CAUSE THEY DON...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>BIHDAY YOUR MAJESTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model love you take with you all the time in ur</td>\n",
       "      <td>MODEL LOVE TAKE WITH ALL THE TIME IN UR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide society now motivation</td>\n",
       "      <td>FACTSGUIDE SOCIETY NOW MOTIVATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49154</th>\n",
       "      <td>thought factory leftright polarisation trump u...</td>\n",
       "      <td>THOUGHT FACTORY LEFTRIGHT POLARISATION TRUMP U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49155</th>\n",
       "      <td>feeling like mermaid hairflip neverready forma...</td>\n",
       "      <td>FEELING LIKE MERMAID HAIRFLIP NEVERREADY FORMA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49156</th>\n",
       "      <td>hillary campaigned today in ohioomg used words...</td>\n",
       "      <td>HILLARY CAMPAIGNED TODAY IN OHIOOMG USED WORDS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49157</th>\n",
       "      <td>happy at work conference right mindset leads t...</td>\n",
       "      <td>HAPPY AT WORK CONFERENCE RIGHT MINDSET LEADS T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49158</th>\n",
       "      <td>my song so glad free download shoegaze newmusi...</td>\n",
       "      <td>MY SONG SO GLAD FREE DOWNLOAD SHOEGAZE NEWMUSI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49159 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clean_tweet  \\\n",
       "0      when father is dysfunctional and is so selfish...   \n",
       "1      thanks for lyft credit cannot use cause they d...   \n",
       "2                                    bihday your majesty   \n",
       "3        model love you take with you all the time in ur   \n",
       "4                      factsguide society now motivation   \n",
       "...                                                  ...   \n",
       "49154  thought factory leftright polarisation trump u...   \n",
       "49155  feeling like mermaid hairflip neverready forma...   \n",
       "49156  hillary campaigned today in ohioomg used words...   \n",
       "49157  happy at work conference right mindset leads t...   \n",
       "49158  my song so glad free download shoegaze newmusi...   \n",
       "\n",
       "                                   clean_tweet_uppercase  \n",
       "0      WHEN FATHER IS DYSFUNCTIONAL AND IS SO SELFISH...  \n",
       "1      THANKS FOR LYFT CREDIT CANT USE CAUSE THEY DON...  \n",
       "2                                    BIHDAY YOUR MAJESTY  \n",
       "3                MODEL LOVE TAKE WITH ALL THE TIME IN UR  \n",
       "4                      FACTSGUIDE SOCIETY NOW MOTIVATION  \n",
       "...                                                  ...  \n",
       "49154  THOUGHT FACTORY LEFTRIGHT POLARISATION TRUMP U...  \n",
       "49155  FEELING LIKE MERMAID HAIRFLIP NEVERREADY FORMA...  \n",
       "49156  HILLARY CAMPAIGNED TODAY IN OHIOOMG USED WORDS...  \n",
       "49157  HAPPY AT WORK CONFERENCE RIGHT MINDSET LEADS T...  \n",
       "49158  MY SONG SO GLAD FREE DOWNLOAD SHOEGAZE NEWMUSI...  \n",
       "\n",
       "[49159 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['clean_tweet', new_target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract raw text \n",
    "new_text = extract_text(df, target=new_target)\n",
    "\n",
    "# Assign tags for tokens\n",
    "new_nltk_tags = nltk.pos_tag(nltk.word_tokenize(new_text))\n",
    "\n",
    "\n",
    "# Search for named entities via  nltk.ne_chunk()\n",
    "new_named_entities = [(' '.join(c[0] for c in chunk), chunk.label() ) \n",
    "                  for chunk in nltk.ne_chunk(new_nltk_tags) \n",
    "                  if hasattr(chunk, 'label') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_cntr = Counter()\n",
    "nltk_ents_dict = {}\n",
    "\n",
    "# Counts all entities\n",
    "\n",
    "for entity in new_named_entities:\n",
    "    nltk_cntr[entity[0]] += 1\n",
    "    if entity[0] not in nltk_ents_dict:\n",
    "        nltk_ents_dict[entity[0]] = entity[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('THE', 4378),\n",
       " ('FATHERS', 983),\n",
       " ('FAMILY', 710),\n",
       " ('TIME', 582),\n",
       " ('HAPPINESS', 556),\n",
       " ('GOLD', 447),\n",
       " ('WELL', 405),\n",
       " ('TO', 380),\n",
       " ('WORLD', 307),\n",
       " ('WHITE', 302)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most common from all entities\n",
    "nltk_cntr.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Counts top-20 Named entities  \n",
    "\n",
    "tmp_cntr = 0\n",
    "\n",
    "\n",
    "nltk_popular_orgs_and_persons = []\n",
    "for i, common in enumerate(nltk_cntr.most_common(10000)):\n",
    "    word = common[0]\n",
    "    count = common[1]\n",
    "    ent_label = nltk_ents_dict[word]\n",
    "    if ent_label == \"PERSON\" or ent_label == \"ORG\" or ent_label == \"GPE\":\n",
    "        nltk_popular_orgs_and_persons.append((word, ent_label, count))\n",
    "        \n",
    "    if len(nltk_popular_orgs_and_persons) == TOP_LIMIT:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CLINTON', 'PERSON', 21),\n",
       " ('CLICK', 'PERSON', 9),\n",
       " ('CLICK TO WATCH', 'PERSON', 8),\n",
       " ('ARABIC', 'PERSON', 8),\n",
       " ('RUSSIA', 'GPE', 7),\n",
       " ('CLICK RECIPE', 'PERSON', 6),\n",
       " ('AMERICANS', 'GPE', 5),\n",
       " ('JOHNSON', 'PERSON', 5),\n",
       " ('BELGIAN', 'GPE', 4),\n",
       " ('JOHN', 'PERSON', 4),\n",
       " ('CLINTONS', 'PERSON', 4),\n",
       " ('INDIAN', 'GPE', 4),\n",
       " ('CLICK TO', 'PERSON', 3),\n",
       " ('JOHN WOODEN', 'PERSON', 3),\n",
       " ('ELECT', 'PERSON', 3),\n",
       " ('JOHN BURR', 'PERSON', 2),\n",
       " ('ALBUM ON', 'PERSON', 2),\n",
       " ('STEIN', 'PERSON', 2),\n",
       " ('SOROS', 'PERSON', 2),\n",
       " ('JOHN MCCAIN TO', 'PERSON', 2)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resulted top-20 named entities\n",
    "nltk_popular_orgs_and_persons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER via SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">when father is dysfunctional and is so selfish he drags his kids into his dysfunction run thanks for lyft credit cannot use cause they do not offer wheelchair vans in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    pdx\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " disapointed \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    getthanked\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " bihday your majesty model love you take with you all the time in ur factsguide society now motivation huge fan fare and big talking before they leave chaos and pay disputes when they get there allshowandnogo camping \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    tomorrow\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    danny\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the next school year\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " is \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the year\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " for exams cannot think about that school exams hate imagine actorslife revolutionschool girl we won love the land allin \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    cavs champions\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " cleveland clevelandcavaliers welcome here am it has it is so \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    gr ireland\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " consumer price index mom climbed from previous to in may blog silver gold forex we are so selfish \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    orlando\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " standwithorlando pulseshooting orlandoshooting \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    biggerproblems\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " selfish heabreaking values love get to see my daddy \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    today days\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " gettingfed \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    cnn\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " calls \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    michigan\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " middle school build the wall chant tcot no comment in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    australia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " opkillingb</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_txt = text[:1000]\n",
    "article = nlp(t)\n",
    "displacy.render(article, jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"e01aa86264194c93807ae7652c6668ec-0\" class=\"displacy\" width=\"925\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">when</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">father</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">dysfunctional</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e01aa86264194c93807ae7652c6668ec-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,2.0 400.0,2.0 400.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e01aa86264194c93807ae7652c6668ec-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e01aa86264194c93807ae7652c6668ec-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e01aa86264194c93807ae7652c6668ec-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e01aa86264194c93807ae7652c6668ec-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e01aa86264194c93807ae7652c6668ec-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e01aa86264194c93807ae7652c6668ec-0-3\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e01aa86264194c93807ae7652c6668ec-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display text dependences\n",
    "displacy.render(article[:5], style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_string_entities(text_string, cntr, ents_dict):\n",
    "    doc = nlp(text_string)\n",
    "    ents = [(e.text, e.label_) for e in doc.ents]\n",
    "    for entity in ents:\n",
    "\n",
    "        cntr[entity[0]] += 1\n",
    "        if entity[0] not in ents_dict:\n",
    "            ents_dict[entity[0]] = entity[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cntr = Counter()\n",
    "ents_dict = {}\n",
    "\n",
    "# Counts all entities\n",
    "\n",
    "for i in range(len(df)):\n",
    "    parse_string_entities(df.iloc[i]['clean_tweet'], cntr, ents_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('today', 1350) DATE\n",
      "('friday', 590) DATE\n",
      "('tomorrow', 586) DATE\n",
      "('one', 526) CARDINAL\n",
      "('first', 514) ORDINAL\n",
      "('orlando', 482) GPE\n",
      "('sunday', 472) DATE\n",
      "('morning', 436) TIME\n",
      "('bihday', 420) DATE\n",
      "('tonight', 407) TIME\n",
      "('summer', 400) DATE\n",
      "('saturday', 324) DATE\n",
      "('monday', 250) DATE\n",
      "('america', 212) GPE\n",
      "('night', 192) TIME\n",
      "('two', 187) CARDINAL\n",
      "('days', 182) DATE\n",
      "('london', 172) GPE\n",
      "('weekend', 169) DATE\n",
      "('thursday', 162) DATE\n"
     ]
    }
   ],
   "source": [
    "# Top-20 cited entities\n",
    "for word in cntr.most_common(TOP_LIMIT):\n",
    "    ent_label = ents_dict[word[0]]\n",
    "    print(word, ent_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Counts top-20 Named entities  \n",
    "\n",
    "tmp_cntr = 0\n",
    "\n",
    "\n",
    "popular_orgs_and_persons = []\n",
    "for i, common in enumerate(cntr.most_common(10000)):\n",
    "    word = common[0]\n",
    "    count = common[1]\n",
    "    ent_label = ents_dict[word]\n",
    "    if ent_label == \"PERSON\" or ent_label == \"ORG\" or ent_label == \"GPE\":\n",
    "        popular_orgs_and_persons.append((word, ent_label, count))\n",
    "        \n",
    "    if len(popular_orgs_and_persons) == TOP_LIMIT:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('orlando', 'GPE', 482),\n",
       " ('america', 'GPE', 212),\n",
       " ('london', 'GPE', 172),\n",
       " ('us', 'GPE', 126),\n",
       " ('bing bong bing bong', 'PERSON', 114),\n",
       " ('sjw', 'ORG', 106),\n",
       " ('uk', 'GPE', 97),\n",
       " ('obama', 'PERSON', 94),\n",
       " ('allahsoil', 'ORG', 88),\n",
       " ('florida', 'GPE', 82),\n",
       " ('gop', 'ORG', 81),\n",
       " ('miami', 'GPE', 74),\n",
       " ('suppo', 'PERSON', 68),\n",
       " ('nba', 'ORG', 63),\n",
       " ('india', 'GPE', 63),\n",
       " ('japan', 'GPE', 57),\n",
       " ('trump', 'ORG', 56),\n",
       " ('usa', 'GPE', 54),\n",
       " ('hillary', 'PERSON', 53),\n",
       " ('nyc', 'ORG', 52)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_orgs_and_persons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распознавание именованных сущеностей в **Nltk** сильно зависит от регистра написания, капризный к тексту. Слова в нижнем регистре не обрабатываются. В топе поисков - имена, фамилии людей, крупные географические имена собственные. Есть ошибочное (мусорное) детектирование\n",
    "\n",
    "Модуль **Spacy** обрабатывает именованные сущности в любом регистре, по этой причине чаще может ошибаться на оммонимах (us != US). В топе детекций, в основном имена географических объектов. Присутствуют ошибочные (мусорные) детекции. В целом, Spacy более гибок к входным данным, однако, требуется дополнительная подгонка под документ и оценка опечаток \n",
    "\n",
    "В целом, обе бибилиотеки пригодны для использования только в связке с постпроцессорами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
