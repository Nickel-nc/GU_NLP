{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осуществим предобработку данных с Твиттера, чтобы отчищенный данные в дальнейшем использовать для задачи классификации. Данный датасет содержит негативные (label = 1) и нейтральные (label = 0) высказывания.\n",
    "Для работы объединим train_df и test_df.\n",
    "\n",
    "Задания:\n",
    "\n",
    "1) Заменим html-сущности (к примеру: &lt; &gt; &amp;). \"&lt;\" заменим на “<” и \"&amp;\" заменим на “&”)\"\"\". Сделаем это с помощью HTMLParser.unescape(). Всю предобработку делаем в новом столбце 'clean_tweet'\n",
    "\n",
    "2) Удалим @user из всех твитов с помощью паттерна \"@[\\w]*\". Для этого создадим функцию: \n",
    " - для того, чтобы найти все вхождения паттерна в тексте, необходимо использовать re.findall(pattern, input_txt)\n",
    " - для для замены @user на пробел, необходимо использовать re.sub()\n",
    "при применении функции необходимо использовать np.vectorize(function).\n",
    "\n",
    "3) Изменим регистр твитов на нижний с помощью .lower().\n",
    "\n",
    "4) Заменим сокращения с апострофами (пример: ain't, can't) на пробел, используя apostrophe_dict. Для этого необходимо сделать функцию: для каждого слова в тексте проверить (for word in text.split()), если слово есть в словаре apostrophe_dict в качестве ключа (сокращенного слова), то заменить ключ на значение (полную версию слова).\n",
    "\n",
    "5) Заменим сокращения на их полные формы, используя short_word_dict. Для этого воспользуемся функцией, используемой в предыдущем пункте.\n",
    "\n",
    "6) Заменим эмотиконы (пример: \":)\" = \"happy\") на пробелы, используя emoticon_dict. Для этого воспользуемся функцией, используемой в предыдущем пункте.\n",
    "\n",
    "7) Заменим пунктуацию на пробелы, используя re.sub() и паттерн r'[^\\w\\s]'.\n",
    "\n",
    "8) Заменим спец. символы на пробелы, используя re.sub() и паттерн r'[^a-zA-Z0-9]'.\n",
    "\n",
    "9) Заменим числа на пробелы, используя re.sub() и паттерн r'[^a-zA-Z]'.\n",
    "\n",
    "10) Удалим из текста слова длиной в 1 символ, используя ' '.join([w for w in x.split() if len(w)>1]).\n",
    "\n",
    "11) Поделим твиты на токены с помощью nltk.tokenize.word_tokenize, создав новый столбец 'tweet_token'.\n",
    "\n",
    "12) Удалим стоп-слова из токенов, используя nltk.corpus.stopwords. Создадим столбец 'tweet_token_filtered' без стоп-слов.\n",
    "\n",
    "13) Применим стемминг к токенам с помощью nltk.stem.PorterStemmer. Создадим столбец 'tweet_stemmed' после применения стемминга.\n",
    "\n",
    "14) Применим лемматизацию к токенам с помощью nltk.stem.wordnet.WordNetLemmatizer. Создадим столбец 'tweet_lemmatized' после применения лемматизации.\n",
    "\n",
    "15) Сохраним результат предобработки в pickle-файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Common Preprocessing Pipeline\"\"\"\n",
    "\n",
    "# 1. clear text\n",
    "# 2. drop stop words & to_lower()\n",
    "# 3. tokenize (char, words, sentences)\n",
    "# 4. stemming, lemmatize\n",
    "# 5. encode, vectorize: (tfidf, CountVectorizer, HashingVectorizer)\n",
    "# 6. Deploy\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# Import libs\n",
    "##############\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re  # regex will be common in python soon\n",
    "import html\n",
    "import nltk\n",
    "import pickle\n",
    "from utils.dicts import apostrophe_dict, emoticon_dict, short_word_dict\n",
    "\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"  # handle multiple outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# Settings\n",
    "###########\n",
    "\n",
    "STOP_WORDS = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "STEMMER = nltk.stem.PorterStemmer()\n",
    "LEMMATIZER = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "# Data load\n",
    "train_df = pd.read_csv('data/train_tweets.csv')\n",
    "test_df = pd.read_csv('data/test_tweets.csv')\n",
    "df = train_df.append(test_df, ignore_index = True, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1    0.0   @user when a father is dysfunctional and is s...\n",
       "1   2    0.0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3    0.0                                bihday your majesty\n",
       "3   4    0.0  #model   i love u take with u all the time in ...\n",
       "4   5    0.0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49159 entries, 0 to 49158\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   id      49159 non-null  int64  \n",
      " 1   label   31962 non-null  float64\n",
      " 2   tweet   49159 non-null  object \n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Preprocess Routine\n",
    "######################\n",
    "###################\n",
    "\n",
    "\n",
    "def replace_from_dict(text, source_dict):\n",
    "    \n",
    "    \"\"\"Search through text, map tokens via dict, place default if not in dict\"\"\"\n",
    "    \n",
    "    return \" \".join([source_dict.get(word, word) for word in text.split()])\n",
    "\n",
    "\n",
    "def remove_onechar_tokens(text):\n",
    "    \n",
    "    \"\"\"Search through text, remove one-caracter tokens\"\"\"\n",
    "    \n",
    "    return ' '.join([w for w in text.split() if len(w)>1])\n",
    "\n",
    "\n",
    "def filter_stop_words(tokens, stop_words=STOP_WORDS):\n",
    "    \n",
    "    \"\"\"Remove stop words from tokens\"\"\"\n",
    "    \n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "\n",
    "def stem_tokens(tokens, stemmer=STEMMER):\n",
    "    \n",
    "    \"\"\"Stemming preprocessing\"\"\"\n",
    "    \n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "\n",
    "def lemmatize_tokens(tokens, lemmatizer=LEMMATIZER):\n",
    "    \n",
    "    \"\"\"Lemmatize preprocessing\"\"\"\n",
    "    \n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "\n",
    "def preprocess(df,\n",
    "               src_col='tweet',\n",
    "               clean_col='clean_tweet',\n",
    "               token_col='tweet_token',\n",
    "               filter_col='tweet_token_filtered',\n",
    "               stemmed_col='tweet_stemmed',\n",
    "               lem_col='tweet_lemmatized'\n",
    "              ):\n",
    "     \n",
    "        \n",
    "    # 1. Clean html context\n",
    "    df[clean_col] = df[src_col].apply(lambda x: html.unescape(x))\n",
    "    \n",
    "    # 2. Remove @user references\n",
    "    df[clean_col] = df[clean_col].apply(lambda x: re.sub(r'@[\\w]*','', x))\n",
    "    \n",
    "    # 3. Correct register to lowercase\n",
    "    df[clean_col] = df[clean_col].str.lower()\n",
    "    \n",
    "    # 4. Change apostrophes\n",
    "    vfunc = np.vectorize(replace_from_dict)\n",
    "    df[clean_col] = vfunc(df[clean_col], apostrophe_dict)\n",
    "    \n",
    "    # 5. Prolong short words\n",
    "    df[clean_col] = vfunc(df[clean_col], short_word_dict)\n",
    "    \n",
    "    # 6. Replace emoticons\n",
    "    df[clean_col] = vfunc(df[clean_col], emoticon_dict)\n",
    "    \n",
    "    # 7. Replace punctuation to spaces\n",
    "    df[clean_col] = df[clean_col].apply(lambda x: re.sub(r'[^\\w\\s]','', x))\n",
    "    \n",
    "    # 8. Replace special characters to spaces\n",
    "    df[clean_col] = df[clean_col].apply(lambda x: re.sub(r'[^a-zA-Z0-9]', ' ', x))\n",
    "    \n",
    "    # 9. Replace nums for spaces\n",
    "    df[clean_col] = df[clean_col].apply(lambda x: re.sub(r'[^a-zA-Z]', ' ', x))\n",
    "    \n",
    "    # 10. Drop one char words\n",
    "    vfunc = np.vectorize(remove_onechar_tokens)\n",
    "    df[clean_col] = vfunc(df[clean_col])\n",
    "    \n",
    "    # 11. Tokenize text\n",
    "    df[token_col] = df[clean_col].apply(lambda x:  nltk.tokenize.word_tokenize(x))\n",
    "    \n",
    "    # 12. Filter stop words\n",
    "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    df[filter_col] = df[token_col].apply(lambda x: filter_stop_words(x))\n",
    "    \n",
    "    # 13. Apply stemming\n",
    "    df[stemmed_col] = df[filter_col].apply(lambda x: stem_tokens(x))\n",
    "    \n",
    "    # 14. Lemmatize\n",
    "    df[lem_col] = df[stemmed_col].apply(lambda x: lemmatize_tokens(x))\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 42.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "result = preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Save to_pickle\n",
    "result.to_pickle(\"output/df_processed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
